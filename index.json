[{"content":"","date":"December 15 2022","permalink":"/","section":"","summary":"","title":""},{"content":"","date":"December 15 2022","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"The annual AWS re:Invent conference has come and gone. As usual, there is an overwhelming amount of new product launches, feature enhancements, and other service offering announcements to parse through. You could spend several days just sifting through all of the information on \u0026rsquo;new stuff\u0026rsquo;. What I was interested this time, however, was some of the announcements for security-related services and features, especially those that solve pain points I\u0026rsquo;ve experienced in the past.\nAmazon Security Lake - Preview # This one seemed to get all the press and attention from the jump, and for good reason - it\u0026rsquo;s an easy way to centralize data from security-related sources in both AWS and your on-premises environment. They\u0026rsquo;re doing some automatic conversions to Open Cybersecurity Schema Framework for supposed interoperability. A number of AWS services like Route 53, CloudTrail, Lambda, S3, Security Hub, Guard Duty, and Macie will support this right away, but you can also create your own source integrations as long as you give your source the ability to write to Security Lake and invoke AWS Glue with the following policies:\nAWSGlueServiceRole Managed Policy The following inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::aws-security-data-lake-{region}-xxxx/*\u0026#34; ] } ] } However, the aspect of this that really caught my eye was the ecosystem of third-party integrations they already available for use in the Preview. Security lake has source integrations for a lot of commonly used services like Ping Identity, Okta, Orca, CrowdStrike, Zscaler, and more - but the subscriber integrations is even more interesting. Sure, you can integrate with Splunk or Sumo Logic, but there\u0026rsquo;s also large consulting firms such as PwC, Deloitte, Accenture, and others that offer to do the analysis and anomaly detection for you. Security Lake seems like it could be a boon to firms that act as Managed Security Service Providers (MSSPs) by really streamlining the ability to aggregate and provide access to data from an organization\u0026rsquo;s disparate systems.\nCloudWatch Logs Sensitive Data Protection # This one is for anyone who has tried to run an App Sec program and get various application teams to go back and fix their logging. Instead of trying to convince teams to spend precious sprint cycles on fixing logging, you can just set up a data protection policy in the application\u0026rsquo;s CloudWatch Log Group and specify the data that you want to have redacted.\nThis approach doesn\u0026rsquo;t have to be something you continuously come back and check on either - you can have alarms that fire on the LogEventsWithFindings metric that will tabulate how many times sensitive information was redacted in a log group. That metric could also be useful if you want to show improvement across teams as you burn down this particular area of risk. Additionally, you can offload reports of these findings to another log group, S3, or through Kinesis to the destination of your choosing.\nCloudWatch Cross-Account Observability # AWS accounts are often treated as an isolation boundary in organizations, with individual teams having some level of control over their own account, even if they are part of the same AWS organization. However, there may be times when you want to implement some form of log or telemetry data capture from many accounts without imposing an unncessary burden on them with bespoke tooling or excessive permissions.\nCross-Account Observability in CloudWatch sets out to solve exactly that problem by allowing you to designate a central \u0026ldquo;monitoring account\u0026rdquo; and one or more \u0026ldquo;source accounts\u0026rdquo; that will feed the monitoring account data and logs. Instead of having to manually implement some form of regular data capture-and-forward, CloudWatch will do the plumbing for you, after you provide the list of source accounts and opt-in to the sharing from each source account.\nOne caveat with this feature - it will only work for the region it is configured in. If you span multiple regions in the source accounts, you\u0026rsquo;ll have to configure this in each region to feed into the monitoring account in all of those regions.\nVPC Lattice - Preview # I\u0026rsquo;ve been in a lot of AWS networking discussions that involve some combination of VPC Peering, Transit Gateways, Private Endpoints, and VPN attachments. Depending on the requirements, there\u0026rsquo;s often at least one good answer and design that can be solutioned out, but it will often come with some drawbacks - additional infrastructure to set up and manage, additional safeguards that have to be put into place, or even a rearrangement of existing resources in terms of VPC and subnet topology.\nVPC Lattice hopes to provide another solution for this type of problem by implementing a logical service network to abstract away the realities of networking and allow services in different accounts and VPCs to talk to each other via DNS. If the picture below reminds you of ELBs, you\u0026rsquo;re not wrong - a lot of the same terminology and principals apply.\nThere\u0026rsquo;s listeners that dictate what type of traffic is expected; those listeners have rules with priority and conditions to dictate which actions to take to forward traffic to the appropriate group of targets. The really nice part about this service, however, is that you can associate an IAM resource policy with individual services in the network to only allow certain services and principals access to designated services.\nIt\u0026rsquo;s networking without networking - and yet it\u0026rsquo;s all still networking.\nKMS External Key Store # AWS services that aim to encrypt data at rest use a key for their specific service, known as the data encryption key. However, because the service needs access to that key, it has to remain with the service itself. But this setup means that the key likely lives next to the data it\u0026rsquo;s protecting - if this one area of storage is compromised, everything is lost.\nTo solve this problem, the data encryption key is itself encrypted by a root key that the customer manages in AWS Key Management Service (KMS). The root key is generated and stored in a hardware security module (HSM) that is tamper resistant. The key material never leaves the HSM. This works fine if you are using KMS to manage your root key and only need to encrypt and decrypt data keys for AWS services - but what happens if you want to integrate with services that don\u0026rsquo;t live on AWS and don\u0026rsquo;t have any connectivity to KMS?\nThat\u0026rsquo;s where AWS KMS External Key Store (XKS) comes into play. Instead of using AWS KMS and forcing every service to talk to it, your root key can be generated and remain inside an HSM outside of AWS. For services that live outside of AWS, they can talk directly to this external HSM as they normally would. But what about services you may still be using that reside in AWS?\nWith XKS, these AWS services will still make their API calls to AWS KMS; however, KMS will be aware that an External Key Store is configured, and instead forward the requests to your external HSM via an XKS proxy. This proxy is designed to translate the AWS KMS API request to a format that the external HSM expects. This setup lets you run services both inside and outside of AWS with just one location for your root keys that can remain firmly under your control.\nSummary # That\u0026rsquo;s just a drop in the bucket of announcements. There was also improvements to managing controls in Control Tower, Verified Access Preview (which I have yet to dig into) that aims to allow for secure remote access without a VPN, and more improvements for finding sensitive data in S3 with Macie. Hopefully I\u0026rsquo;ll have time to try out each of them before reInforce sneaks up on me later this year.\n","date":"December 15 2022","permalink":"/posts/2022-12-15/","section":"Posts","summary":"The annual AWS re:Invent conference has come and gone.","title":"AWS re:Invent 2022 Security Recap"},{"content":"","date":"December 15 2022","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"December 15 2022","permalink":"/categories/cloud-security/","section":"Categories","summary":"","title":"Cloud Security"},{"content":"","date":"December 15 2022","permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking"},{"content":"","date":"December 15 2022","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"December 15 2022","permalink":"/tags/security/","section":"Tags","summary":"","title":"Security"},{"content":"","date":"December 15 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"August 16 2022","permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"","date":"August 16 2022","permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"MongoDB"},{"content":" Disclaimer: I am a MongoDB employee, and this is a lightning talk I gave at MongoDB World 2022.\nAs your application continues to grow and scale, you may choose to take advantage of powerful MongoDB Atlas features, such as multi-region clusters and sharding, in order to provide a good user experience. While these features are useful, they can also introduce complexities to your application’s architecture if configured incorrectly. In this session, we’ll look at common architectures when designing multi-region sharded clusters and walk through how MongoDB Atlas allows your team to securely connect to each of the clusters without exposing unnecessary components to the public internet.\n","date":"August 16 2022","permalink":"/posts/2022-08-16/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee, and this is a lightning talk I gave at MongoDB World 2022.","title":"MongoDB World 2022 Talk: Look Ma, No Public IP!"},{"content":"","date":"August 16 2022","permalink":"/categories/talks/","section":"Categories","summary":"","title":"Talks"},{"content":"","date":"May 30 2022","permalink":"/categories/appsec/","section":"Categories","summary":"","title":"AppSec"},{"content":"","date":"May 30 2022","permalink":"/categories/hardware/","section":"Categories","summary":"","title":"Hardware"},{"content":"","date":"May 30 2022","permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":"May 30 2022","permalink":"/tags/raspberry-pi/","section":"Tags","summary":"","title":"Raspberry Pi"},{"content":"Disclaimer: I am a MongoDB employee.\nBackground # Several years back, I received a Raspberry Pi 3 Model B+ kit from CanaKit alongside this book. I spent some time doing the basic stuff with it (blinking LEDs, running a Linux server, etc.) but eventually turning it into a RetroPie and installing lots of retro games on it. After a while, I lost interest in tinkering with it and just let it collect dust in my office for a number of years.\nI was recently cleaning out my office and rediscovered both the old RaspberryPi and the book, and was thumbing through the projects when I noticed one that caught my eye. Listed in the book as Project 12, it\u0026rsquo;s a simple temperature and humidity data logger that didn\u0026rsquo;t really draw my interest a few years ago. Since then, however, I\u0026rsquo;ve moved to a new place where the bedrooms are essentially on the third floor (making them much warmer than the rest of the house). Because of this setup, one of the things that my wife and I constantly ask each other is how warm or cold it is in my son\u0026rsquo;s nursery compared to the downstairs portions of the house. With this in mind, I decided to see if I could combine this simple Raspberry Pi project with the new Time Series collections that MongoDB offers starting with version 5.0. The idea was to make a tool that would display the current conditions of my son\u0026rsquo;s nursery while also giving me the ability to show how the conditions changed throughout the day and look for patterns if I felt the need to do so.\nAcquiring the Hardware # To build the temperature and humidity sensor, the book lists the following components:\nRaspberry Pi (obviously) Breadboard DHT22 temperature and humidity sensor (can substitute DHT11 or AM2302 as well) 4.7k ohm resistor Jumper wires That\u0026rsquo;s not an overwhelming number of components, but it\u0026rsquo;s still a bunch of pieces that will make for a pretty messy package if you\u0026rsquo;re trying to leave it somewhere in a discreet manner. You would have to connect GND on the Pi to the breadboard\u0026rsquo;s blue rail, 3.3V on the Pi to the breadboard\u0026rsquo;s red rail, and then connect the sensor to the breadboard and Pi as well, using the resistor on the 3.3V connection for DHT22 pin 2.\nInstead, I found this DHT22 sensor that comes with a handy 3-pin wire that you can plug directly into the Pi. DOUT goes to GPIO (pin 4) on the Pi, VCC goes to pin 1 on the Pi, and GND goes to pin 6 on the Pi. Using the case that came with my Pi from CanaKit, I was able to enclose the Pi and just had the wires escape out the top to the sensor, greatly cleaning up the overall appearance.\nBooting the Pi # This was by far the easiest part of this project. The Raspberry Pi OS (formerly known as Raspbian) gives you an easy to use Linux OS. Better yet, the Imager utility helps you set up your install so it will work exactly the way you want right out of the box. You can setup a user for SSH so once it powers up, you can connect directly to it in a headless fashion. Once you have your Pi booted up and are able to SSH into it, the real work begins.\nCreating the Time Series Database # We\u0026rsquo;ll be using the Time Series collections feature of MongoDB 5.0. The easiest way to get started is to spin up a free tier cluster on MongoDB Atlas and then connect in with the new mongo shell, mongosh. You\u0026rsquo;ll also want to make sure you create a database user with credentials that the script can use to connect to the database, as well as add an entry on the IP Access List to allow the connection in the first place.\nOnce connected to your cluster through mongosh, you\u0026rsquo;ll want to create the time series collection as follows:\nuse ts db.createCollection(\u0026#34;homeclimate\u0026#34;, { timeseries: { timeField: \u0026#34;timestamp\u0026#34;, metaField: \u0026#34;metadata\u0026#34;, granularity: \u0026#34;seconds\u0026#34; }, expireAfterSeconds: 604800 } ) The above command does the following:\nCreates a time series collection called \u0026lsquo;homeclimate\u0026rsquo; in the \u0026rsquo;ts\u0026rsquo; database Establishes a granularity of seconds for document ingestion Tells MongoDB that the \u0026rsquo;timestamp\u0026rsquo; field will be used to represent the time of each reading, and the \u0026lsquo;metadata\u0026rsquo; field will hold the information used to identify where the reading is coming from (such as what sensor) Makes documents in this collection expire after 604800 seconds or a little over 7 days, as we don\u0026rsquo;t want to pay for an ever increasing data size and any data older than that is probably of low analytical value Scripting Data Collection # The projects book linked to the Adafruit Python DHT library but if you go to the Github repo, you\u0026rsquo;ll see that it\u0026rsquo;s deprecated. The new version is CircuitPython which can be easily installed on the Raspberry Pi OS using pip:\npip3 install adafruit-circuitpython-dht From there, it\u0026rsquo;s time to actually start pulling data from the sensor with a continuously running script. Create a new Python script and edit it (I recommend using Visual Studio Code in Remote Mode to make this easier) to contain the following:\nimport time import board import adafruit_dht import datetime import pymongo # Initial the dht device, with data pin connected to: dhtDevice = adafruit_dht.DHT22(board.D4) # Set up python client for MongoDB client = pymongo.MongoClient(\u0026lt;MONGODB CONNECTION STRING\u0026gt;) db = client.ts collection = db.homeclimate sensor = 1 while True: try: # Pull the values right from the sensor device temperature_c = dhtDevice.temperature temperature_f = temperature_c * (9 / 5) + 32 humidity = dhtDevice.humidity # Write a document with the temperature and humidity to the time series collection document = {\u0026#34;metadata\u0026#34;: {\u0026#34;sensorId\u0026#34;: sensor, \u0026#34;type\u0026#34;: \u0026#34;climate\u0026#34;}, \u0026#34;timestamp\u0026#34;: datetime.datetime.utcnow(), \u0026#34;temperature_fahrenheit\u0026#34;: temperature_f, \u0026#34;temperature_celsius\u0026#34;: temperature_c, \u0026#34;humidity\u0026#34;: humidity } doc_id = collection.insert_one(document).inserted_id print(\u0026#34;Recorded reading to document {}\u0026#34;.format(doc_id)) time.sleep(10) except RuntimeError as error: # Errors will happen but need to keep going. Can make up for missed readings # in the Time Series collection print(error.args[0]) time.sleep(2.0) continue except Exception as error: dhtDevice.exit() raise error time.sleep(10.0) There\u0026rsquo;s a few things going on in this script:\nFirst, we\u0026rsquo;re setting up the actual sensor by using the driver to build a device object and saying its connected on pin 4 (GPIO) on the Raspberry Pi. We\u0026rsquo;re creating a connection to our MongoDB cluster and specifying the database and collection we\u0026rsquo;ll be storing data in. We also setup this sensor\u0026rsquo;s identifier. Since this was my first sensor, I gave it a sensor ID of 1. Future sensors will increment this value. In a infinite while loop, we grab the temperature and humidity values from the sensor. Since I live in the US, I also convert this temperature to farenheit and put both temperature values alongside with humidity into a MongoDB document. Since we\u0026rsquo;re using a Time Series collection, I have an embedded document with my metadata values - a sensorID and a sensor type that may come in handy in the future as I scale these out. The only other value in the document is the current UTC timestamp, which is critical for helping build the time series view. After taking the reading and writing it to the database, we wait ten seconds until we do it again. This time can be adjusted up or down depending on how granular you want your time series data to be. If you\u0026rsquo;ve configured your MongoDB connection string correctly and added the entry in the IP Access List in Atlas, running this script should start writing to your Time Series collection. You should see the results after just a few seconds by going to the Collections view in Atlas:\nUsing MongoDB Charts to Visualize the Data # Now that the readings are coming into MongoDB, you can use the Charts feature of MongoDB Atlas to create a dashboard and display useful information and visualizations derived from the raw data. Start by clicking on the Charts tab at the top of MongoDB Atlas, then select Data Sources in the left hand menu. On that page, click the Add Data Source button in the upper right and then select your cluster. It should only have one database - ts - and one collection - homeclimate. Select those and click Finish to set up the data source.\nNow that the time series collection shows up in the Data Sources list, look to the Pipeline column near the right hand side. Click the Add Pipeline button. We\u0026rsquo;re going to use the $setWindowFields aggregation operator to build a window function on the time series collection; specifically, we\u0026rsquo;re going to run a calculation on only the readings from the last hour.\nInside the Aggregation Pipeline Edit modal that appears, paste in the following pipeline:\n[ {$setWindowFields: { partitionBy: \u0026#34;$metadata.sensorId\u0026#34;, sortBy: { timestamp: 1}, output: { lastHourAverageTemp: { $avg: \u0026#34;$temperature_fahrenheit\u0026#34;, window: { documents: [-360, 0] } }, lastHourAverageHumidity: { $avg: \u0026#34;$humidity\u0026#34;, window: { documents: [-360, 0] } } } } } ] This window function creates two new fields for us to use in MongoDB Charts - the lastHourAverageTemp and lastHourAverageHumidity. Note that I used the original Fahrenheit temperature as the source for this new average temperature field - depending on where you\u0026rsquo;re located, you may want to swap in Celsius instead. Since we\u0026rsquo;re collecting readings every 10 seconds in our original script, we go back 360 readings (6 readings a minute multipled by 60 minutes in an hour). Click Save when you\u0026rsquo;ve pasted this in.\nNow click on Dashboards on the left hand navigation, then select the Add Dashboard button on the far right. Call the dashboard whatever you want. Once it\u0026rsquo;s open, we\u0026rsquo;ll have to add some charts. Click on Add Chart and you\u0026rsquo;ll be brought to the MongoDB Charts editor. If you\u0026rsquo;ve never used it before, I recommend checking out the Charts documentation to check out all the features it has.\nFor now, we\u0026rsquo;re going to add a simple chart to display the current temperature. In the upper left under data source, select ts.homeclimate which should be the only option available. Choose Text as the Chart Type, then select Top Item as want the most recent temperature reading. Now drag \u0026rsquo;timestamp\u0026rsquo; to the Sort field placeholder, and click the sort button to the right of it to make sure it\u0026rsquo;s in descending order. Drag temperature_fahrenheit to the Display field placeholder and you should now get the most recent reading\u0026rsquo;s Fahrenheit temperature value. Click Save and Close at the top.\nNow we can add another chart and do the same exact thing, but instead of the regular \u0026rsquo;temperature_fahrenheit\u0026rsquo; field being used as the reading, we can use the \u0026rsquo;lastHourAverageTemp\u0026rsquo; field that was added via the pipeline we attached to the data source. This will give us a view of the average temperature of the last hour in case the most recent temperature becomes an outlier, like if someone opens a window or starts running a heater.\nThen we can add a third chart to visualize the change in temperature over time from the sensor. Add a new chart and this time, for Chart Type select Line. In the Encode tab, for the X-Axis select timestamp and turn on Binning, selecting Hour from the dropdown. For the Y-Axis select temperature_fahrenheit, and under the Aggregate dropdown select Mean. The combination of these two settings will combine all of an hour\u0026rsquo;s readings into one group and take the average to represent that hour on the line chart. Finally, under Series we can select sensorId under metadata. While we only have one sensor currently, if we add more sensors in the future, this will show different lines for each sensor.\nNow on the Filter tab, drag timestamp in as the filter and select Period, Previous, and 3 Day to limit the chart to only consider the previous three days worth of data. Again, you can adjust this period up or down to suit your needs.\nIf you save and close, you\u0026rsquo;ll have a nice dashboard of three charts showing you some basic temperature inforamtion from your sensor. As your sensor continues writing data, the line chart will become more useful as well. You can repeat this process to create three charts with the humidity data as well to give you a full representation of readings from the sensor.\n","date":"May 30 2022","permalink":"/posts/2022-05-30/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee.","title":"Tracking Temperature and Humidity at Home with Time Series Data"},{"content":"Disclaimer: I am a MongoDB employee.\nMost developers by now are familiar with MongoDB, a NoSQL database that stores data as documents instead of rows. MongoDB\u0026rsquo;s fully managed service product, MongoDB Atlas, comes with MongoDB Realm, which is a set of services that help facilitate mobile and web development by providing a scalable, serverless backend for your application. Realm offers a lot of services (more than we can cover in just this post), but today I wanted to focus on how to use two of them in tandem to help connect a MongoDB Atlas database into a distributed, event driven architecture that can be built on AWS.\nPre-requisites # Before we get started, there are a couple of things you should have setup already. These include:\nAn AWS account with an IAM user created, and an access key created for that IAM user. The user should have permissions to create, edit, and retrieve parameters from AWS Systems Manager and put objects, list buckets, and get objects from S3.\nA MongoDB Atlas account. It is free to sign up, and there is a free tier that you can use to follow along with this post. The Community version of MongoDB does not include MongoDB Realm, so this post does not apply to it.\nEmulating an application environment # We\u0026rsquo;re going to have to setup a few things to make this look and feel like an environment a real application is using. Most modern applications are not just writing data to a database and reading it back; rather, they are consuming events from/publishing events to a message queue or stream, interacting with cloud storage, and relying on slight changes to data to kick off workflows. To keep things simple, our goal will be to monitor our database for new documents that are inserted and to upload a copy of them to S3 at the exact moment they are written to the database. In our theoretical application, we are using the copy of the document to do some additional processing or analysis with an AWS service that is reading from S3.\nSet up MongoDB Cluster # Log into your MongoDB Atlas account. Create a project to act as the container for your cluster, then create a cluster. If you want to use the free tier, choose the Shared category at the top of the Create Cluster page. Use a recommended, free-tier region such as us-east-1 or us-east-2 and then scroll down select the M0 Sandbox Cluster Tier. Change the name of the cluster from Cluster0 to Sandbox and click on Create Cluster.\nSet up the S3 Bucket # While the MongoDB cluster is creating, head over to the AWS console and login as your IAM user. Go to Amazon S3 in the AWS console and choose Create bucket. Give it a name like {yourname}-event-bucket and select the same region that you created your MongoDB Atlas cluster in. Leave the other options as the default selections and click Create bucket at the bottom of the page.\nSet up parameters in SSM # In the AWS console, search for AWS Systems Manager (SSM) and navigate to the service page. Once there, select Parameter Store on the left hand navigation, under Application Management. Select Create Parameter.\nOn the Create parameter page, enter event-target-bucket as the name for the parameter. Leave the Tier as Standard, the Type as String, and the Data type as text. Under value, enter the name of the S3 bucket you created in the previous step.\nConfiguring MongoDB Atlas and Realm # Load Data Into Cluster # Return to the MongoDB Atlas console. When your sandbox is finished provisioning, click the \u0026hellip; button and choose Load Sample Dataset. This will give us a few databases and collections of data to use for our testing purposes.\nSet up Realm App # While that data is being loaded, click on the Realm tab near the top of the screen. We want to select Create a New App and name it AWS-Event-App. Under \u0026ldquo;Link your Database\u0026rdquo;, select Use an existing MongoDB Atlas Data Source and choose the Sandbox cluster you created. Then click Create Realm Application.\nSet up Realm Values and Secrets # In order to hook into our AWS account and use the bucket and the parameter we just set up, we have to authenticate to the account from MongoDB Realm. Since we\u0026rsquo;ll be using our AWS IAM user\u0026rsquo;s access key and secret key, we don\u0026rsquo;t want to hardcode it into any code we end up writing or reading from source control. Therefore, we will use Realm\u0026rsquo;s Values feature to securely store these values.\nRealm\u0026rsquo;s Values feature allows you to store two types of values - regular values and secrets. Secrets cannot be directly accessed by the Realm API; instead, they must be mapped to their own regular value in order to be able to be retrieved. This prevents you from inadvertantly exposing a secret you did not intend to.\nClick on Values in the left hand navigation of your Realm app. Click on Create New Value. For the value\u0026rsquo;s name, enter AccessKeyID. Choose Value as the type, then for Content select Custom Content and paste in your Access Key wrapped in double quotations to mark it as a string. Then click on Save Draft.\nClick on Create New Value again. Enter SecretAccessKeySecret as the value name, and this time select Secret under Type. For the value of secret, paste in your AWS IAM user\u0026rsquo;s Secret Access Key, then click Save Draft.\nWe\u0026rsquo;ve entered the Secret Access Key, but we cannot directly access it via the Realm API since it is a secret. We now have to create a value and map it to that secret. Click on Create New Value a third time. Enter SecretAccessKey and select Value as the type. Under Add Content, choose Link to Secret and select the SecretAccessKeySecret you just created. Click Save Draft.\nWe have one more value to set up before we\u0026rsquo;re done. Click on Create New Value again, and this time enter Region as the value name. Leave Type as Value and Add Content as Custom Content. In the content field, enter the AWS region where you set up your S3 bucket and SSM parameter, again wrapped in double quotations. Then click Save Draft.\nYou may ask why we bothered with SSM at all to store the parameter of the S3 bucket name. It is true that we could also store the bucket name here in Realm\u0026rsquo;s values; however, for the scope of the post we will assume the application environment has already been using SSM for parameters such as these, and therefore it will be less work for us to leave it that way.\nCreate Realm Functions # Now we are ready to start writing some code. First, let\u0026rsquo;s bring in the AWS SDK which we will use in our code to make calls to the specific AWS services we will use. Select Functions in the left hand navigation then click the Dependencies tab at the top of the page. Select Add Dependency. On the modal that appears, enter aws-sdk as the package name. You can leave the version blank, but if you run into issues, you may want to come back and change the version to 2.737.0, which is what I used to write this post. When you\u0026rsquo;re done, click Add.\nThe aws-sdk node library should be added as a dependency. Now click Create New Function button in the upper right. In the Add Function page, enter MoveDocToQueue as the function and for now choose System under Authentication. Click on the Function Editor tab at the top, and paste this code in over what is currently there.\nexports = async function(event){ const AWS = require(\u0026#39;aws-sdk\u0026#39;); const config = { accessKeyId: context.values.get(\u0026#34;AccessKeyID\u0026#34;), secretAccessKey: context.values.get(\u0026#34;SecretAccessKey\u0026#34;), region: context.values.get(\u0026#34;Region\u0026#34;) }; const SSMparams = { Name: \u0026#39;event-target-bucket\u0026#39;, WithDecryption: false }; const doc = JSON.stringify(event.fullDocument); let SSM = new AWS.SSM(config); const ssmPromise = await SSM.getParameter(SSMparams).promise(); bucketName = ssmPromise.Parameter.Value; const S3params = { Bucket: bucketName, Key: \u0026#34;queue-\u0026#34; + event.fullDocument._id, Body: doc }; let S3 = new AWS.S3(config); const s3Promise = S3.putObject(S3params).promise(); s3Promise.then(function(data) { console.log(\u0026#39;Put Object Success\u0026#39;); }).catch(function(err) { console.log(err); }); }; Normally with Realm functions, you can click on Run near the bottom right to test out the function. However, since this function will require a change event in the database to run, we will need to map it to a trigger first and modify some data to test it out. For now, click on Save Draft.\nWe now have the actual code we want to run to move documents to our S3 bucket. But how do we get it to run whenever we insert data? The answer is a Realm Trigger. Select Triggers on the left hand navigation, then add a new trigger.\nOn the Add Trigger page, enter moveDocToQueueTrigger as the name, then select your Sandbox cluster under Trigger Source Details. For database, select sample_mflix and for collection select movies. Under Operation type, ensure Insert is checked as we want this trigger to fire on every new document inserted into the sample_mflix.movies collection. Keep scrolling down and ensure that Full Document is turned to ON. Under Select an Event Type, choose Function and select the MoveDocToQueue function that we just created. Then click Save.\nDeploying and Testing # We are now ready to try our first test! On the blue banner at the top of the page, you can now choose to Deploy the changes to your Realm app. The values, secrets, function, and trigger should now be ready to go. Open up a connection to your MongoDB cluster and insert a new document into the sample_mflix.movies collection. You can do this via the mongo shell, a MongoDB driver, MongoDB Compass, or just by using the Data Explorer built into Atlas to duplicate an existing document in that collection. Keep in mind that if you are using the shell, a driver, or Compass, you will need to add your current IP address to the IP Access list in your Atlas project.\nAfter you have inserted a new document, return to MongoDB Realm and select Logs in the left hand navigation. You should an entry in the History showing when your trigger was invoked. Expanding the entry will show you the logs from that particular function - if you set up your values and secrets correctly, you should see the log message \u0026ldquo;Put Object Success\u0026rdquo;, meaning that our attempt to put the document into the S3 bucket appears to be successful. (If you do not see this output, double check your values and secrets, your IAM user permissions, and your S3 and SSM configuration.)\nLet\u0026rsquo;s double check that this actually sent the document to S3 by going back to our AWS console and navigating to the S3 service page. Find the bucket that you created at the beginning of this process and open it up. You should see an object in there starting with queue- and then the _id value of the document you inserted. The object is named this way because we set this as the Key value when setting up the S3 parameters in our Realm function. If you see the document, then your setup is successful!\nYou can continue experimenting by bringing in other AWS services as well, such as sending the document to a Kinesis data stream instead of just an S3 bucket.\n","date":"March 20 2022","permalink":"/posts/2022-03-20/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee.","title":"Building Event Driven Experiences with MongoDB Realm and AWS"},{"content":"","date":"March 20 2022","permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"NodeJS"},{"content":"About # Hello world!\nI\u0026rsquo;m John Misczak and this is my blog where I (infrequently) write about some stuff I\u0026rsquo;m working on or topics I\u0026rsquo;m currently learning about.\nI\u0026rsquo;ve taken on a number of different roles in my career, including in identity management, penetration testing, application security, threat modeling, and cloud security. I\u0026rsquo;ve even dipped my toe into the sales world for a bit to help customers build cool stuff. Currently, my focus is on helping teams build applications and services securely in public cloud environments.\nI always enjoy talking to new people so please reach out if you would like to say hello.\n","date":"January 1 0001","permalink":"/about/","section":"","summary":"About # Hello world!","title":""},{"content":"Contact Me # There are a few ways you can get in touch with me to discuss any of the topics I\u0026rsquo;ve written about here or anything else that comes to mind.\nEmail # john.misczak [at] gmail.com\nMastodon # https://infosec.exchange/@misczak\nLinkedIn # https://www.linkedin.com/in/johnmisczak\n","date":"January 1 0001","permalink":"/contact/","section":"","summary":"Contact Me # There are a few ways you can get in touch with me to discuss any of the topics I\u0026rsquo;ve written about here or anything else that comes to mind.","title":""},{"content":"","date":"January 1 0001","permalink":"/archive/","section":"","summary":"This page contains an archive of all posts.","title":"Archive"}]