[{"content":"","date":"August 18 2023","permalink":"/","section":"/misczak","summary":"","title":"/misczak"},{"content":"","date":"August 18 2023","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"August 18 2023","permalink":"/categories/cloud-security/","section":"Categories","summary":"","title":"Cloud Security"},{"content":"","date":"August 18 2023","permalink":"/tags/gcp/","section":"Tags","summary":"","title":"GCP"},{"content":"Introduction # Earlier this year, my team was provided access to a training budget for Google Cloud Platform that we could use in various ways to purchase classroom trainings or licenses to Google\u0026rsquo;s on-demand training platform, Cloud Skills Boost as well as a number of exam vouchers that could be used on various GCP certification exams. While I had previously worked almost exclusively with AWS (only using GCP for some testing of private service connect), I knew that I had some upcoming projects that required me to build services on GCP. For that reason, I jumped at the opportunity to take advantage of this training and hopefully prepare myself enough to write an exam at the end of it.\nUnbeknownst to me at the time, Google was also offering us a number of seats in what they call their Certification Journey. This program is essentially a focused, six week schedule to work through the Cloud Skills Boost content for your selected certification, with a few extra bonuses layered in. After doing some of the introductory Cloud Skills Boost content and building on GCP myself for a few months, I decided to enroll in the Certification Journey for the Professional Cloud Security Engineer certification. My cohort was scheduled to start in the middle of May, wrapping up by the end of June.\nPreparing for the Exam # The Certification Journey was an interesting program that lays somewhere between formal classroom SANS training an the open free-for-all that is regular Udemy courses with Discords attached. First, you don\u0026rsquo;t undertake the program alone; you are scheduled into a cohort with others who are aiming for the same certification at that time, and have opted into the program themselves. There\u0026rsquo;s a Google Group set up for discussion among members of the cohort on topics covered in the program, although mine was pretty sparsely used.\nThe other interesting wrinkle to the Certification Journey program is that your cohort is assigned an instructor, who holds group \u0026ldquo;office hours\u0026rdquo; once a week for 90 minutes to go over that week\u0026rsquo;s content, walk you through sample questions similar to those found on the exam, and most importantly explain to you why the correct answer is the right answer. I was skeptical of these sessions initially, but soon came to find them incredibly valuable. Our instructor pointed out pitfalls I had missed when reading documentation or playing around in GCP myself - such as the fact that BigQuery has its data access logs enabled by default, the only service in GCP to do so.\nThe Cloud Skills Boost content is, for the most part, of a very high quality. This content is accessible outside the Certification Journey program with just a regular license that runs $29 per month or $300 per year. There are some lectures in there that are less interesting than others, but they\u0026rsquo;re usually kept to byte-sized videos (4-7 minutes in length) that make it easy to fit in between other tasks or meetings in your day. The real value of Cloud Slills Boost, however, are the labs, as they spin up an ephemeral project for you to create resources and mess around in. Once the lab is complete, they tear down the project and any resources within. It\u0026rsquo;s great for peace of mind, as you don\u0026rsquo;t have to worry about making sure you disposed of every resource you used in a lab that could end up billing you.\nIn addition to all this content, each week there was a series of links to documentation, blog posts, and videos for further reading and watching. I held off on watching these until I got through most of the Cloud Skills Boost content, knowing that each one would be part of a deeper dive into a service or feature. In the last week or so before my exam, I was working through these links non-stop and watching any video I could find on each service. There are some services I would probably never get the chance to play around with hands on (like setting up Cloud Interconnect for the first time), but I felt prepared for everything else.\nInterestingly, I didn\u0026rsquo;t feel the need to do a practice exam, as the sample questions from the instructor office hours gave me a good preview of what the questions from the exam itself would be like. These sample questions were scenario based, with each possible answer often being a series of steps and actions you would take in GCP - not just simple recall of a service\u0026rsquo;s name or feature. The questions also leaned away from any \u0026ldquo;gotchas\u0026rdquo; where the right answer is surrounded by wrong answers that are just spelled differently, or something similar. This setup made me feel confident that I wouldn\u0026rsquo;t have to memorize the exact form of every IAM predefined role or setting, as just knowing what role types would have what permissions would suffice.\nSitting the Exam # I\u0026rsquo;ve done a couple of certification exams before, and the registration and check-in process was pretty similar to those. The exam itself was 45 questions, and I flew through the first ten questions before finding the next thirty-five much more detailed and challenging. However, I didn\u0026rsquo;t have any questions where I was completely at a loss to answer - instead, if I wasn\u0026rsquo;t 100% sure of my answer, I marked it for review and moved on. At the end of my first pass of the exam, I had about nineteen questions marked for review and did another loop through those, which whittled it down to nine questions I still was 50/50 on between two answers. I spent some additional time on those and then submitted my exam. Total time was about one hour, ten minutes.\nResults # The test results screen immediately showed that I had achieved a provisional pass, but would have to wait for an email from Google Cloud for confirmation. This took about a day and a half to arrive, with links to setup a profile for my badge on Accredible. There was also a token to use in the swag store for those who passed Professional tier GCP exams, which allowed me to have a certification welcome kit to be mailed to me. I\u0026rsquo;ve heard in years past this used to be a hoodie or something similar, but the only option on the store available to me was the thermal mug seen in the picture below.\nAll in all, I would say my experience of preparing for and achieving this certification was a positive one. The material did not feel endless, instead really focusing in on the best ways to set up secure organizations and projects in GCP. I came to appreciate the format of the questions, which avoided focusing on what I would call trivia and instead emphasizing really understanding the sequence of steps and nuance needed for good security.\n","date":"August 18 2023","permalink":"/posts/2023-08-18/","section":"Posts","summary":"Introduction # Earlier this year, my team was provided access to a training budget for Google Cloud Platform that we could use in various ways to purchase classroom trainings or licenses to Google\u0026rsquo;s on-demand training platform, Cloud Skills Boost as well as a number of exam vouchers that could be used on various GCP certification exams.","title":"Google's Professional Cloud Security Engineer Certification"},{"content":"","date":"August 18 2023","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"August 18 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Background # One of AWS\u0026rsquo; best practices for building and managing infrastructure in the cloud is to use consistent, accurate tags for the purposes of cost management, correct owner attribution (during operations and security incidents), and even attribute-based access control. This type of tag compliance can be easier said than done, however, especially when dealing with an organization that has a number of accounts owned by different teams with very different tooling and working styles.\nMy team has dealt with a number of security incidents over the last year where finding the correct owner of an EC2 instance took too much time, in our eyes, at the beginning of the incident. No tags were added to the instance besides Name, and while CloudTrail would capture the the events that created the resource (and the principal involved), some resources predated our currently retained CloudTrail logs - meaning we did not have a record of the user that created them.\nWhile it is possible to require tags to be added to resources at the time of creation (otherwise blocking the creation of the resource), my team preferred not to pursue this approach for a few reasons. Implementing that type of compliance check would take a very long time to incorporate into the various workflows across the dozens of accounts for which we have oversight. Moreover, restricting resource creation until somebody enters the correct tag information can make the security team look like blockers instead of team players, which is something we are always looking to avoid.\nFor these reasons, I wanted to see what could be done to leverage information AWS has about our resources in order to attach tags about the owner to the instance\u0026rsquo;s metadata itself without, so it would live alongside the instance for the duration of its lifetime. The preferred approach would not require any additional work on behalf of the individual or team creating the resources in the first place, and not interfere with their existing workflows.\nResearching Previous Work # A lot of times when trying to work on projects such as this one, I end up with the thought process that somebody, somewhere, has to have solved this particular problem before. So I began my research process across the Internet, Reddit, and even the Cloud Security Slack.\nWhat became immediately apparent is that AWS already has this information available to you; however, it\u0026rsquo;s just not in a format (or part of a service) that makes it easy to extract. There is a tag called aws:createdBy that can be activated in the Billing Console via the management account, after which AWS will apply it to a subset of resources.\nThe only issue with this tag is that it is only available in the Billing console and reports - it does not appear alongside other tags for your resources. There are some great projects out there that look to make this report information more easily queryable - but they still didn\u0026rsquo;t feel like the right fit for us, as many of the projects pushed the tag information into a separate repository, with tag information about newly created resources susceptible to a delay.\nFinally, I found a post from the AWS Cloud Operations \u0026amp; Migrations blog from back in November 2020 that was exactly what I was looking for. It was a solution that used CloudWatch events and a Lambda function to read incoming RunInstances CloudTrail events and use the information from within the event to tag the newly created resource. It even had sample Python code for the Lambda function! There were only one problem with this example that prevented it from being a slam dunk - it was focused on only one account and one region. To make this solution useful in our environment, we would have to extend it to work across all of the accounts in a certain OU, and across all regions.\nThe Auto Tagging Solution, Explained # In case you don\u0026rsquo;t want to click through to the AWS blog post itself, I\u0026rsquo;ll briefly explain the solution. When a principal starts an EC2 instance in AWS, a RunInstances event is generated and logged by CloudTrail. These events contain a variety of information, including the time of the event, the principal that triggered it, the account that the instance was created in, and the region that the instance was created in. All of this information is extremely useful to capture.\nA service called AWS EventBridge (formerly CloudWatch Events) allows for rules to be created that can monitor CloudTrail for specific events, and then perform some action based on the event. The auto tagging solution includes a rule that monitors for the RunInstances events, and then sends the information from a captured event to an AWS Lambda function. This Lambda function reads the information in the event (mostly the instance ID and principal information), and then uses that information to apply a tag to the instance with the information about the principal.\nThe chief limitation of this solution, as presented in the blog post, is that CloudTrail in a given region will only capture the RunInstances events that happen in that region. Additionally, a Lambda function would only be able to apply tags to instances in the same account as the function itself.\nModifying the AWS Example # The first problem - that the example solution only covered a single region - can be solved easily enough. Whereas the example used a single Event Rule, we would have to end up deploying the same rule in every region (of every account) that we planned on supporting as part of this solution. This requirement could be met by running a CloudFormation StackSet that created the rule in each region that each account uses.\nThe second problem - that the example focused on just one account - was a little trickier to think about. Event rules can send events to Lambda functions only in the same account as the rule. We needed a way to send events from other accounts in the organization to our account, and then another mechanism to apply the tags back onto the correct instances in those various accounts.\nWe ended up creating an event bus in our Security account, solely for the purpose of receiving these resource creation events from other accounts. Each event rule would send their events to this event bus instead of directly to a Lambda function. The bus, in turn, would be the doorway to our Lambda function, as it would live in the same account as the function itself.\nNext, we created two IAM roles in each managed account. One role has the PutEvent permission, used by the Event Rule to send events to the event bus in the Security account. The second role has the ec2:CreateTags and ec2:DescribeVolumes permission, and its trust policy allows our Lambda\u0026rsquo;s execution role to assume it. CloudFormation is again used to help deploy these to all accounts in scope. It\u0026rsquo;s best to deploy the CloudFormation for creating the IAM roles first, as you can then reference the PutEvent role in your CloudFormation for deploying the Event Rule itself.\nLastly, we added permissions to our Lambda execution role; namely, a policy to assume the auto tagging role in any account:\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::*:role/auto-tag-role\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;ResourceAutoTaggerAssumeRole\u0026#34; } ], \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34; } Once the Lambda function was invoked, it would mostly use the code provided by the AWS blog post example. However, we modified a couple of key areas. We used the source code from the blog post example GitHub repo and set about modifying resource-auto-tager.py. In the lambda_handler function, we added some code to pull the account ID and region from the CloudTrail event:\n# Parse the passed CloudTrail event and extract pertinent EC2 launch fields event_fields = cloudtrail_event_parser(event) accountId = event_fields.get(\u0026#34;account_id\u0026#34;) region = event_fields.get(\u0026#34;region\u0026#34;) log.info(f\u0026#34;Instance created in Account: {accountId} in region: {region}\\n\u0026#34;) We then pass accountID and region to the set_ec2_instance_attached_vols_tags function, alongside the ec2_instance_id and resource_tags as seen in the example. Once inside that function, we use the account_id to assume the IAM role that we deployed to every account for the purposes of applying the tags, and build an EC2 client object on the back of that assumed role session and the region.\ndef set_ec2_instance_attached_vols_tags(ec2_instance_id, resource_tags, account_id, region): try: # First assume the role created in the account that has permission to tag the resources log.info(\u0026#34;Attempting to assume role in target account\\n\u0026#34;) assumed_role_response = sts_client.assume_role( RoleArn=f\u0026#34;arn:aws:iam::{account_id}:role/auto-tag-role\u0026#34;, RoleSessionName=\u0026#34;auto-tag-session\u0026#34; ) assumed_role_session = boto3.Session(aws_access_key_id=assumed_role_response[\u0026#39;Credentials\u0026#39;][\u0026#39;AccessKeyId\u0026#39;], aws_secret_access_key=assumed_role_response[\u0026#39;Credentials\u0026#39;][\u0026#39;SecretAccessKey\u0026#39;], aws_session_token=assumed_role_response[\u0026#39;Credentials\u0026#39;][\u0026#39;SessionToken\u0026#39;]) assumed_role_ec2_client = assumed_role_session.client(\u0026#34;ec2\u0026#34;, region_name=region) log.info(\u0026#34;Assumed role in target account successfully\\n\u0026#34;) Because we have used CloudFormation to deploy the same role to every account, it will be waiting for us, no matter which account_id gets passed in here and used as part of the ARN. And because the Lambda\u0026rsquo;s execution role is trusted by each of those roles in the various accounts, it will be able to assume it without issue. The rest of the code is almost entirely unmodified.\nStepping back and looking at the complete solution, it can be summarized by this diagram:\nApplying the Service Control Policy # Once the tags are created, we want to prevent someone from removing them, either accidentally or in an attempt to cover their tracks. This can be handled by a pretty straightforward service control policy (SCP) that just blocks the tags that you have decided to apply. In this case, we denied the ec2:DeleteTags action for the owner and dateCreated tags, and attached it to the OU of accounts where we would deploy the solution.\ndata \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;auto_tag_delete_policy\u0026#34; { statement { effect = \u0026#34;Deny\u0026#34; actions = [ \u0026#34;ec2:DeleteTags\u0026#34; ] resources = [\u0026#34;*\u0026#34;] condition { test = \u0026#34;ForAnyValue:StringEquals\u0026#34; variable = \u0026#34;aws:TagKeys\u0026#34; values = [ \u0026#34;dateCreated\u0026#34;, \u0026#34;owner\u0026#34; ] } } } resource \u0026#34;aws_organizations_policy\u0026#34; \u0026#34;prevent_auto_tag_delete\u0026#34; { name = \u0026#34;prevent-auto-tag-modification\u0026#34; description = \u0026#34;Prevents removal of tags automatically applied by InfoSec\u0026#34; content = data.aws_iam_policy_document.auto_tag_delete_policy.json } resource \u0026#34;aws_organizations_policy_attachment\u0026#34; \u0026#34;managed_ou\u0026#34;{ policy_id = aws_organizations_policy.prevent_auto_tag_delete.id target_id = var.managed_ou } Results and Additional Considerations # Once everything was in place and enabled, tags began being automatically applied to our EC2 instances as they were created.\nThrough an extensive testing and monitoring process, I learned a few things and wanted to point out some additional things to keep in mind:\nRegions further away from where your event bus and Lambda function are will take a few seconds longer to have tags applied to them, due to round trip times. Make sure you adjust the default Lambda timeout threshold to something like 3 minutes just to make sure you\u0026rsquo;re not losing any tagging due to a slightly delayed event. While the SCP prevents someone from deleting the tags, it does nothing to stop them from modifying them. Overwriting tags falls under the ec2:CreateTags permission, which is what is used to apply these tags in the first place. Therefore, it would be trivial for someone to edit the owner tag after the fact to cover their own tracks. It is recommended you combine this solution with a tool or service that takes routine inventory snapshots of your instance fleet, such as CloudQuery so that you can track changes to tags over time. We actually ended up adding an additional tag called \u0026ldquo;WhatIsThis\u0026rdquo; with a value that was a URL that pointed to a page on our internal security wiki explaining what auto-tagging was and why it was being applied to resources. While we sent out communications to the account owners and administrators prior to rolling the solution out, there was still going to be some users who would be confused why certain tags were being applied to instances that they had created. Depending on your organization\u0026rsquo;s culture, it may be a good idea to include something like this to prevent a torrent of emails or Slack messages inquiring about the tags. If your organization has teams already using infrastructure as code to manage their resources, they may need to add some configuration to ignore these tags as sources of drift. For example, Terraform has an ignore_tags configuration block for the AWS provider that will allow the automatically applied tags to be ignored. For accounts that routinely spin up large numbers of EC2 instances at the same time, make sure you review the current rate limits and quotas for the DescribeInstances, CreateTags, and DescribeVolumes API calls, as you can quickly hit the rate limit when large numbers of resources are created concurrently. Although this solution works great for newly created EC2 instances, it does nothing for instances that already exist. For those use cases, you\u0026rsquo;re probably better off using something like Mark Wolfe\u0026rsquo;s excellent project that automates pushing Cost and Usage report information into an Athena table. It is my hope that this post helps others fill in this gap in their own AWS security and compliance posture. I\u0026rsquo;m always happy to discuss this solution, especially ways it can be improved, if you wish to reach out.\n","date":"July 31 2023","permalink":"/posts/2023-07-31/","section":"Posts","summary":"Background # One of AWS\u0026rsquo; best practices for building and managing infrastructure in the cloud is to use consistent, accurate tags for the purposes of cost management, correct owner attribution (during operations and security incidents), and even attribute-based access control.","title":"Automatically tagging resources in AWS with Owner Information"},{"content":"","date":"July 31 2023","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"Recently, I had to complete a project that involved running the open source tool Cloudquery to create an inventory of resources in a GCP organization. This assignment turned out to be a great introduction to learning how Google Cloud Platform works, as I had almost exclusively used AWS previously (with only minor trials in both Azure and GCP during that time). Throughout the project, I found myself often mapping certain concepts back to what they would be called or how they would be done in AWS. In doing so, I started to keep score of what I liked more than AWS and what I liked less - and now I\u0026rsquo;m finally sitting down to organize some of those thoughts.\nWhat I Like More on GCP # 1. The Organizaton-Project-Folder Hierarchy # In GCP, the top node in your environment is called an organization; under that, you can have projects, which act like AWS accounts but tend to be even more logically segmented. The reason for that strict segmentation is that you\u0026rsquo;ll often place projects under folders to better organize them under a team or department or business unit. Projects allow you to easily create and control very fine grained permissions, as you can place just a few resources in a project to limit the blast radius of any permissions granted there - without having to implement lots of IAM policies.\nAWS has a concept of an organization for central management, but it is by no means a requirement to get started. You can still see its DNA as a feature that came later on, as you have to invite existing accounts to the organization. And the management account still runs the show in the organization, so you\u0026rsquo;ll still have one account that quite a bit more powerful than the others.\n2. Almost Every Aspect of IAM # Building on the organization-folder-project hierarchy, the IAM structure in GCP immediately seems more logical once you understand how it was constructed. Every principal in GCP is tied to an email address - a byproduct of an organization requiring a domain to get started. That principal\u0026rsquo;s email address can be added to any organization, folder, or project and granted permissions in that entity. These permissions can cascade from the top of the hierarchy downwards; for example, you can grant permissions at the Engineering folder level and have it apply to that principal for every project underneath it.\nWhere this really becomes powerful (and convenient) is providing different permissions at different levels. For example, I may want to give the service account that my workload will use the \u0026lsquo;Security Reviewer\u0026rsquo; predefined role at the organization level, which grants List permissions for nearly every service as well as Get (Read) permissions for many of them. Those permissions will cascade to every folder and therefore every project in that organization. But then, in a few specific projects, I can add the principal for my service account and grant it more specific, powerful permissions, such as Storage Object Creator to write objects to Cloud Storage Buckets in those projects. Essentially, two CLI commands are needed to set up these different tiers of permissions throughout the organization.\nOn the AWS side, I would most likely need to get roles or trust relationships added to every account in my organization for my workload to use to emulate this setup. And even then, the policies for those roles have to probably be tailored to specific resources because each account contains lots of resources my workload shouldn\u0026rsquo;t have access to.\n3. Less Tedious Networking (For the Most Part) # Some of the biggest differences in comparing GCP to AWS come at the network level; specifically, VPCs in GCP are global and go across regions, while subnets are regional and can go across zones. This allows me to setup a highly available workload spanning multiple regions while using just a single VPC. Additionally, I can spread my subnets across different zones for better fault tolerance. While you can easily emulate this on AWS with multi-region setups, there\u0026rsquo;s some additional effort that goes into designing different VPCs for each region and different subnets for each zone.\nThe benefits become more clear when you realize that in GCP, you don\u0026rsquo;t attach a CIDR block at the VPC level - instead, it\u0026rsquo;s done at the subnet level. Such a setup creates an interesting tradeoff when it comes to firewall rules for these networks; while you can have a firewall rule apply for an entire global VPC and all of its subnets, that firewall rule is inherently tied to that VPC and cannot be repurposed elsewhere, like you can with security groups.\nOne last note here - I love how every subnet comes with an option to enable \u0026lsquo;Private Google Access\u0026rsquo;, which just allows Compute Engine instances in that subnet to reach the external IP addresses of GCP services even if they only have internal IP addresses. This is a really simple option to toggle without having to worry about setting up a VPC endpoint, as you do in AWS.\n4. Focus on CLI commands, Even in the Console # For many pages that I visited in the console to create a resource or change a configuration, there was an option near the bottom that allowed me to see and copy the equivalent CLI command for the changes I had just made. By putting this front and center during console use (as opposed to burying it in a reference page somewhere), it became really simple for me to just copy those commands to my personal notes, making my configuration repeatable in the future if I wanted to replicate it in another project or environment.\nWhat I Like More on AWS # 1. No domain requirement # As I mentioned above, the organization in GCP is tightly coupled with a domain, for good reason - principals and resources become entities under that domain. But this can be a pain when trying to spin up a new environment as a sandbox for your team or if you just want to start a new project that can be brought under the management of an organization at a later date. While it\u0026rsquo;s easy to undestand that every principal is an email address in GCP, I found myself still preferring to just reference a principal by account number and role name. Some of the email addresses for principals in GCP can get extremely long and be very close in terms of spelling and project IDs - I once granted the permission to the the Compute Engine Service Account used for my workload instead of the Compute System Service Agent.\n2. APIs are Already Enabled # The first time you attempt to call the API of a service in a GCP project, you\u0026rsquo;ll have to Enable it which requires you to accept the Terms of Service and billing responsibility for the API. While this is done presumably to prevent a lower privileged user from attempting to use and consume valuable (and costly) resources in your project, it can be extremely counterintuitive, especially when you are trying to engineer and debug tools that are reaching across projects and using lots of different APIs.\nIn the case of using the CloudQuery tool, I was getting lots of inconsistent results - for example, the tool was not returning any data for certain resources, like Cloud Functions, that I knew existed in the organization. You can get around this by either enabling all of the APIs in your project so that they can be used by your workload, or by granting your workload the serviceusage.services.enable permission so it can enable the APIs on its own as it needs to.\n3. No Service Account/Service Agents Confusion # As mentioned in the first item of this section, I once granted the Compute Instance Admin predefined role to a GCP service account in my project instead of the Compute System Service Agent. This role was required to allow my Compute Engine instances to make use of Instance Schedules, which would start and stop them at predefined times.\nThe reason for this mix-up was partially due to the fact that Service Agents are hidden on the IAM page until you toggle the box labeled Include Google-provided role grants. I also found it counterintuitive because every other permission I needed for my workload to function correctly was granted to the service account that I had attached to the Compute Engine instance running the workload.\nI can\u0026rsquo;t recall having a mix-up like this while using AWS; there is a pretty clear map of what permissions need to be granted to which roles and how you can attach that role to compute workloads (instance profile, IRSA, etc.).\n4. The ARN # The Amazon Resource Name, or ARN, is a unique identifier assigned to an AWS resource. An ARN can represent a role, an EC2 instance, an S3 bucket, a VPC, and so on. Maybe it is the fact that I learned AWS first, but I have become so accustomed to looking for the ARN and using that as the identifier for a resource that it felt like my toolbox was missing something without an equivalent in GCP. Instead, for some GCP resources, you\u0026rsquo;ll have a name in the format of something like projects/[PROJECT NAME]/locations/[REGION]/functions/my-function, but that name is not as useful for you in other projects and across the organization. Whereas AWS will often ask you to provide the ARN for a resource in a permissions policy or for a principal in a trust policy, you\u0026rsquo;re more likely to be splitting resources into projects in GCP.\n5. Private Service Connect # While Private Google Access was easier to setup for the use case of a workload in a subnet with only private IPs trying to access Google services, Private Service Connect supports the use case of establishing a one way connection between your VPC to another VPC, whether that is in your organization or another organization - such as those of service providers. This makes it roughly equivalent to AWS\u0026rsquo; Private Link.\nHowever, where Private Link allows you to create some pretty interesting architectures by providing it with peering for transitive routing and sharing of endpoints, Private Service Connect endpoints used for third party services can only be accessed from within the VPC where they are created, and only by subnets in the same region. You cannot use a peering connection to \u0026ldquo;hop\u0026rdquo; to the VPC and region where the Private Service Connect endpoint lives, and then use that to travel further along to your destination in a transitive manner.\nThe one exception to this is that you can use a Cloud VPN to get to the VPC with the Private Service Connect endpoint; in some cases, it may be recommended to use a Cloud VPN between two VPCs if you want to share a Private Service Connect endpoint between them. Obviously, this introduces additional complexity over the Private Link setup required by AWS.\nConclusion # All in all, I\u0026rsquo;ve enjoyed my time building on GCP, as it\u0026rsquo;s been a great learning experience and a useful method of challenging assumptions that I\u0026rsquo;ve developed over time by heavily using AWS. Since I spent some time learning how it does IAM in and out, I feel pretty comfortable using it to support our current project and any future ones that may come up.\n","date":"February 27 2023","permalink":"/posts/2023-02-27/","section":"Posts","summary":"Recently, I had to complete a project that involved running the open source tool Cloudquery to create an inventory of resources in a GCP organization.","title":"Building on GCP"},{"content":"The annual AWS re:Invent conference has come and gone. As usual, there is an overwhelming amount of new product launches, feature enhancements, and other service offering announcements to parse through. You could spend several days just sifting through all of the information on \u0026rsquo;new stuff\u0026rsquo;. What I was interested this time, however, was some of the announcements for security-related services and features, especially those that solve pain points I\u0026rsquo;ve experienced in the past.\nAmazon Security Lake - Preview # This one seemed to get all the press and attention from the jump, and for good reason - it\u0026rsquo;s an easy way to centralize data from security-related sources in both AWS and your on-premises environment. They\u0026rsquo;re doing some automatic conversions to Open Cybersecurity Schema Framework for supposed interoperability. A number of AWS services like Route 53, CloudTrail, Lambda, S3, Security Hub, Guard Duty, and Macie will support this right away, but you can also create your own source integrations as long as you give your source the ability to write to Security Lake and invoke AWS Glue with the following policies:\nAWSGlueServiceRole Managed Policy The following inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3WriteRead\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::aws-security-data-lake-{region}-xxxx/*\u0026#34; ] } ] } However, the aspect of this that really caught my eye was the ecosystem of third-party integrations they already available for use in the Preview. Security lake has source integrations for a lot of commonly used services like Ping Identity, Okta, Orca, CrowdStrike, Zscaler, and more - but the subscriber integrations is even more interesting. Sure, you can integrate with Splunk or Sumo Logic, but there\u0026rsquo;s also large consulting firms such as PwC, Deloitte, Accenture, and others that offer to do the analysis and anomaly detection for you. Security Lake seems like it could be a boon to firms that act as Managed Security Service Providers (MSSPs) by really streamlining the ability to aggregate and provide access to data from an organization\u0026rsquo;s disparate systems.\nCloudWatch Logs Sensitive Data Protection # This one is for anyone who has tried to run an App Sec program and get various application teams to go back and fix their logging. Instead of trying to convince teams to spend precious sprint cycles on fixing logging, you can just set up a data protection policy in the application\u0026rsquo;s CloudWatch Log Group and specify the data that you want to have redacted.\nThis approach doesn\u0026rsquo;t have to be something you continuously come back and check on either - you can have alarms that fire on the LogEventsWithFindings metric that will tabulate how many times sensitive information was redacted in a log group. That metric could also be useful if you want to show improvement across teams as you burn down this particular area of risk. Additionally, you can offload reports of these findings to another log group, S3, or through Kinesis to the destination of your choosing.\nCloudWatch Cross-Account Observability # AWS accounts are often treated as an isolation boundary in organizations, with individual teams having some level of control over their own account, even if they are part of the same AWS organization. However, there may be times when you want to implement some form of log or telemetry data capture from many accounts without imposing an unncessary burden on them with bespoke tooling or excessive permissions.\nCross-Account Observability in CloudWatch sets out to solve exactly that problem by allowing you to designate a central \u0026ldquo;monitoring account\u0026rdquo; and one or more \u0026ldquo;source accounts\u0026rdquo; that will feed the monitoring account data and logs. Instead of having to manually implement some form of regular data capture-and-forward, CloudWatch will do the plumbing for you, after you provide the list of source accounts and opt-in to the sharing from each source account.\nOne caveat with this feature - it will only work for the region it is configured in. If you span multiple regions in the source accounts, you\u0026rsquo;ll have to configure this in each region to feed into the monitoring account in all of those regions.\nVPC Lattice - Preview # I\u0026rsquo;ve been in a lot of AWS networking discussions that involve some combination of VPC Peering, Transit Gateways, Private Endpoints, and VPN attachments. Depending on the requirements, there\u0026rsquo;s often at least one good answer and design that can be solutioned out, but it will often come with some drawbacks - additional infrastructure to set up and manage, additional safeguards that have to be put into place, or even a rearrangement of existing resources in terms of VPC and subnet topology.\nVPC Lattice hopes to provide another solution for this type of problem by implementing a logical service network to abstract away the realities of networking and allow services in different accounts and VPCs to talk to each other via DNS. If the picture below reminds you of ELBs, you\u0026rsquo;re not wrong - a lot of the same terminology and principals apply.\nThere\u0026rsquo;s listeners that dictate what type of traffic is expected; those listeners have rules with priority and conditions to dictate which actions to take to forward traffic to the appropriate group of targets. The really nice part about this service, however, is that you can associate an IAM resource policy with individual services in the network to only allow certain services and principals access to designated services.\nIt\u0026rsquo;s networking without networking - and yet it\u0026rsquo;s all still networking.\nKMS External Key Store # AWS services that aim to encrypt data at rest use a key for their specific service, known as the data encryption key. However, because the service needs access to that key, it has to remain with the service itself. But this setup means that the key likely lives next to the data it\u0026rsquo;s protecting - if this one area of storage is compromised, everything is lost.\nTo solve this problem, the data encryption key is itself encrypted by a root key that the customer manages in AWS Key Management Service (KMS). The root key is generated and stored in a hardware security module (HSM) that is tamper resistant. The key material never leaves the HSM. This works fine if you are using KMS to manage your root key and only need to encrypt and decrypt data keys for AWS services - but what happens if you want to integrate with services that don\u0026rsquo;t live on AWS and don\u0026rsquo;t have any connectivity to KMS?\nThat\u0026rsquo;s where AWS KMS External Key Store (XKS) comes into play. Instead of using AWS KMS and forcing every service to talk to it, your root key can be generated and remain inside an HSM outside of AWS. For services that live outside of AWS, they can talk directly to this external HSM as they normally would. But what about services you may still be using that reside in AWS?\nWith XKS, these AWS services will still make their API calls to AWS KMS; however, KMS will be aware that an External Key Store is configured, and instead forward the requests to your external HSM via an XKS proxy. This proxy is designed to translate the AWS KMS API request to a format that the external HSM expects. This setup lets you run services both inside and outside of AWS with just one location for your root keys that can remain firmly under your control.\nSummary # That\u0026rsquo;s just a drop in the bucket of announcements. There was also improvements to managing controls in Control Tower, Verified Access Preview (which I have yet to dig into) that aims to allow for secure remote access without a VPN, and more improvements for finding sensitive data in S3 with Macie. Hopefully I\u0026rsquo;ll have time to try out each of them before reInforce sneaks up on me later this year.\n","date":"December 15 2022","permalink":"/posts/2022-12-15/","section":"Posts","summary":"The annual AWS re:Invent conference has come and gone.","title":"AWS re:Invent 2022 Security Recap"},{"content":"","date":"December 15 2022","permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking"},{"content":"","date":"December 15 2022","permalink":"/tags/security/","section":"Tags","summary":"","title":"Security"},{"content":"","date":"August 16 2022","permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"","date":"August 16 2022","permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"MongoDB"},{"content":" Disclaimer: I am a MongoDB employee, and this is a lightning talk I gave at MongoDB World 2022.\nAs your application continues to grow and scale, you may choose to take advantage of powerful MongoDB Atlas features, such as multi-region clusters and sharding, in order to provide a good user experience. While these features are useful, they can also introduce complexities to your application’s architecture if configured incorrectly. In this session, we’ll look at common architectures when designing multi-region sharded clusters and walk through how MongoDB Atlas allows your team to securely connect to each of the clusters without exposing unnecessary components to the public internet.\n","date":"August 16 2022","permalink":"/posts/2022-08-16/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee, and this is a lightning talk I gave at MongoDB World 2022.","title":"MongoDB World 2022 Talk: Look Ma, No Public IP!"},{"content":"","date":"August 16 2022","permalink":"/categories/talks/","section":"Categories","summary":"","title":"Talks"},{"content":"","date":"May 30 2022","permalink":"/categories/appsec/","section":"Categories","summary":"","title":"AppSec"},{"content":"","date":"May 30 2022","permalink":"/categories/hardware/","section":"Categories","summary":"","title":"Hardware"},{"content":"","date":"May 30 2022","permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":"May 30 2022","permalink":"/tags/raspberry-pi/","section":"Tags","summary":"","title":"Raspberry Pi"},{"content":"Disclaimer: I am a MongoDB employee.\nBackground # Several years back, I received a Raspberry Pi 3 Model B+ kit from CanaKit alongside this book. I spent some time doing the basic stuff with it (blinking LEDs, running a Linux server, etc.) but eventually turning it into a RetroPie and installing lots of retro games on it. After a while, I lost interest in tinkering with it and just let it collect dust in my office for a number of years.\nI was recently cleaning out my office and rediscovered both the old RaspberryPi and the book, and was thumbing through the projects when I noticed one that caught my eye. Listed in the book as Project 12, it\u0026rsquo;s a simple temperature and humidity data logger that didn\u0026rsquo;t really draw my interest a few years ago. Since then, however, I\u0026rsquo;ve moved to a new place where the bedrooms are essentially on the third floor (making them much warmer than the rest of the house). Because of this setup, one of the things that my wife and I constantly ask each other is how warm or cold it is in my son\u0026rsquo;s nursery compared to the downstairs portions of the house. With this in mind, I decided to see if I could combine this simple Raspberry Pi project with the new Time Series collections that MongoDB offers starting with version 5.0. The idea was to make a tool that would display the current conditions of my son\u0026rsquo;s nursery while also giving me the ability to show how the conditions changed throughout the day and look for patterns if I felt the need to do so.\nAcquiring the Hardware # To build the temperature and humidity sensor, the book lists the following components:\nRaspberry Pi (obviously) Breadboard DHT22 temperature and humidity sensor (can substitute DHT11 or AM2302 as well) 4.7k ohm resistor Jumper wires That\u0026rsquo;s not an overwhelming number of components, but it\u0026rsquo;s still a bunch of pieces that will make for a pretty messy package if you\u0026rsquo;re trying to leave it somewhere in a discreet manner. You would have to connect GND on the Pi to the breadboard\u0026rsquo;s blue rail, 3.3V on the Pi to the breadboard\u0026rsquo;s red rail, and then connect the sensor to the breadboard and Pi as well, using the resistor on the 3.3V connection for DHT22 pin 2.\nInstead, I found this DHT22 sensor that comes with a handy 3-pin wire that you can plug directly into the Pi. DOUT goes to GPIO (pin 4) on the Pi, VCC goes to pin 1 on the Pi, and GND goes to pin 6 on the Pi. Using the case that came with my Pi from CanaKit, I was able to enclose the Pi and just had the wires escape out the top to the sensor, greatly cleaning up the overall appearance.\nBooting the Pi # This was by far the easiest part of this project. The Raspberry Pi OS (formerly known as Raspbian) gives you an easy to use Linux OS. Better yet, the Imager utility helps you set up your install so it will work exactly the way you want right out of the box. You can setup a user for SSH so once it powers up, you can connect directly to it in a headless fashion. Once you have your Pi booted up and are able to SSH into it, the real work begins.\nCreating the Time Series Database # We\u0026rsquo;ll be using the Time Series collections feature of MongoDB 5.0. The easiest way to get started is to spin up a free tier cluster on MongoDB Atlas and then connect in with the new mongo shell, mongosh. You\u0026rsquo;ll also want to make sure you create a database user with credentials that the script can use to connect to the database, as well as add an entry on the IP Access List to allow the connection in the first place.\nOnce connected to your cluster through mongosh, you\u0026rsquo;ll want to create the time series collection as follows:\nuse ts db.createCollection(\u0026#34;homeclimate\u0026#34;, { timeseries: { timeField: \u0026#34;timestamp\u0026#34;, metaField: \u0026#34;metadata\u0026#34;, granularity: \u0026#34;seconds\u0026#34; }, expireAfterSeconds: 604800 } ) The above command does the following:\nCreates a time series collection called \u0026lsquo;homeclimate\u0026rsquo; in the \u0026rsquo;ts\u0026rsquo; database Establishes a granularity of seconds for document ingestion Tells MongoDB that the \u0026rsquo;timestamp\u0026rsquo; field will be used to represent the time of each reading, and the \u0026lsquo;metadata\u0026rsquo; field will hold the information used to identify where the reading is coming from (such as what sensor) Makes documents in this collection expire after 604800 seconds or a little over 7 days, as we don\u0026rsquo;t want to pay for an ever increasing data size and any data older than that is probably of low analytical value Scripting Data Collection # The projects book linked to the Adafruit Python DHT library but if you go to the Github repo, you\u0026rsquo;ll see that it\u0026rsquo;s deprecated. The new version is CircuitPython which can be easily installed on the Raspberry Pi OS using pip:\npip3 install adafruit-circuitpython-dht From there, it\u0026rsquo;s time to actually start pulling data from the sensor with a continuously running script. Create a new Python script and edit it (I recommend using Visual Studio Code in Remote Mode to make this easier) to contain the following:\nimport time import board import adafruit_dht import datetime import pymongo # Initial the dht device, with data pin connected to: dhtDevice = adafruit_dht.DHT22(board.D4) # Set up python client for MongoDB client = pymongo.MongoClient(\u0026lt;MONGODB CONNECTION STRING\u0026gt;) db = client.ts collection = db.homeclimate sensor = 1 while True: try: # Pull the values right from the sensor device temperature_c = dhtDevice.temperature temperature_f = temperature_c * (9 / 5) + 32 humidity = dhtDevice.humidity # Write a document with the temperature and humidity to the time series collection document = {\u0026#34;metadata\u0026#34;: {\u0026#34;sensorId\u0026#34;: sensor, \u0026#34;type\u0026#34;: \u0026#34;climate\u0026#34;}, \u0026#34;timestamp\u0026#34;: datetime.datetime.utcnow(), \u0026#34;temperature_fahrenheit\u0026#34;: temperature_f, \u0026#34;temperature_celsius\u0026#34;: temperature_c, \u0026#34;humidity\u0026#34;: humidity } doc_id = collection.insert_one(document).inserted_id print(\u0026#34;Recorded reading to document {}\u0026#34;.format(doc_id)) time.sleep(10) except RuntimeError as error: # Errors will happen but need to keep going. Can make up for missed readings # in the Time Series collection print(error.args[0]) time.sleep(2.0) continue except Exception as error: dhtDevice.exit() raise error time.sleep(10.0) There\u0026rsquo;s a few things going on in this script:\nFirst, we\u0026rsquo;re setting up the actual sensor by using the driver to build a device object and saying its connected on pin 4 (GPIO) on the Raspberry Pi. We\u0026rsquo;re creating a connection to our MongoDB cluster and specifying the database and collection we\u0026rsquo;ll be storing data in. We also setup this sensor\u0026rsquo;s identifier. Since this was my first sensor, I gave it a sensor ID of 1. Future sensors will increment this value. In a infinite while loop, we grab the temperature and humidity values from the sensor. Since I live in the US, I also convert this temperature to farenheit and put both temperature values alongside with humidity into a MongoDB document. Since we\u0026rsquo;re using a Time Series collection, I have an embedded document with my metadata values - a sensorID and a sensor type that may come in handy in the future as I scale these out. The only other value in the document is the current UTC timestamp, which is critical for helping build the time series view. After taking the reading and writing it to the database, we wait ten seconds until we do it again. This time can be adjusted up or down depending on how granular you want your time series data to be. If you\u0026rsquo;ve configured your MongoDB connection string correctly and added the entry in the IP Access List in Atlas, running this script should start writing to your Time Series collection. You should see the results after just a few seconds by going to the Collections view in Atlas:\nUsing MongoDB Charts to Visualize the Data # Now that the readings are coming into MongoDB, you can use the Charts feature of MongoDB Atlas to create a dashboard and display useful information and visualizations derived from the raw data. Start by clicking on the Charts tab at the top of MongoDB Atlas, then select Data Sources in the left hand menu. On that page, click the Add Data Source button in the upper right and then select your cluster. It should only have one database - ts - and one collection - homeclimate. Select those and click Finish to set up the data source.\nNow that the time series collection shows up in the Data Sources list, look to the Pipeline column near the right hand side. Click the Add Pipeline button. We\u0026rsquo;re going to use the $setWindowFields aggregation operator to build a window function on the time series collection; specifically, we\u0026rsquo;re going to run a calculation on only the readings from the last hour.\nInside the Aggregation Pipeline Edit modal that appears, paste in the following pipeline:\n[ {$setWindowFields: { partitionBy: \u0026#34;$metadata.sensorId\u0026#34;, sortBy: { timestamp: 1}, output: { lastHourAverageTemp: { $avg: \u0026#34;$temperature_fahrenheit\u0026#34;, window: { documents: [-360, 0] } }, lastHourAverageHumidity: { $avg: \u0026#34;$humidity\u0026#34;, window: { documents: [-360, 0] } } } } } ] This window function creates two new fields for us to use in MongoDB Charts - the lastHourAverageTemp and lastHourAverageHumidity. Note that I used the original Fahrenheit temperature as the source for this new average temperature field - depending on where you\u0026rsquo;re located, you may want to swap in Celsius instead. Since we\u0026rsquo;re collecting readings every 10 seconds in our original script, we go back 360 readings (6 readings a minute multipled by 60 minutes in an hour). Click Save when you\u0026rsquo;ve pasted this in.\nNow click on Dashboards on the left hand navigation, then select the Add Dashboard button on the far right. Call the dashboard whatever you want. Once it\u0026rsquo;s open, we\u0026rsquo;ll have to add some charts. Click on Add Chart and you\u0026rsquo;ll be brought to the MongoDB Charts editor. If you\u0026rsquo;ve never used it before, I recommend checking out the Charts documentation to check out all the features it has.\nFor now, we\u0026rsquo;re going to add a simple chart to display the current temperature. In the upper left under data source, select ts.homeclimate which should be the only option available. Choose Text as the Chart Type, then select Top Item as want the most recent temperature reading. Now drag \u0026rsquo;timestamp\u0026rsquo; to the Sort field placeholder, and click the sort button to the right of it to make sure it\u0026rsquo;s in descending order. Drag temperature_fahrenheit to the Display field placeholder and you should now get the most recent reading\u0026rsquo;s Fahrenheit temperature value. Click Save and Close at the top.\nNow we can add another chart and do the same exact thing, but instead of the regular \u0026rsquo;temperature_fahrenheit\u0026rsquo; field being used as the reading, we can use the \u0026rsquo;lastHourAverageTemp\u0026rsquo; field that was added via the pipeline we attached to the data source. This will give us a view of the average temperature of the last hour in case the most recent temperature becomes an outlier, like if someone opens a window or starts running a heater.\nThen we can add a third chart to visualize the change in temperature over time from the sensor. Add a new chart and this time, for Chart Type select Line. In the Encode tab, for the X-Axis select timestamp and turn on Binning, selecting Hour from the dropdown. For the Y-Axis select temperature_fahrenheit, and under the Aggregate dropdown select Mean. The combination of these two settings will combine all of an hour\u0026rsquo;s readings into one group and take the average to represent that hour on the line chart. Finally, under Series we can select sensorId under metadata. While we only have one sensor currently, if we add more sensors in the future, this will show different lines for each sensor.\nNow on the Filter tab, drag timestamp in as the filter and select Period, Previous, and 3 Day to limit the chart to only consider the previous three days worth of data. Again, you can adjust this period up or down to suit your needs.\nIf you save and close, you\u0026rsquo;ll have a nice dashboard of three charts showing you some basic temperature inforamtion from your sensor. As your sensor continues writing data, the line chart will become more useful as well. You can repeat this process to create three charts with the humidity data as well to give you a full representation of readings from the sensor.\n","date":"May 30 2022","permalink":"/posts/2022-05-30/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee.","title":"Tracking Temperature and Humidity at Home with Time Series Data"},{"content":"Disclaimer: I am a MongoDB employee.\nMost developers by now are familiar with MongoDB, a NoSQL database that stores data as documents instead of rows. MongoDB\u0026rsquo;s fully managed service product, MongoDB Atlas, comes with MongoDB Realm, which is a set of services that help facilitate mobile and web development by providing a scalable, serverless backend for your application. Realm offers a lot of services (more than we can cover in just this post), but today I wanted to focus on how to use two of them in tandem to help connect a MongoDB Atlas database into a distributed, event driven architecture that can be built on AWS.\nPre-requisites # Before we get started, there are a couple of things you should have setup already. These include:\nAn AWS account with an IAM user created, and an access key created for that IAM user. The user should have permissions to create, edit, and retrieve parameters from AWS Systems Manager and put objects, list buckets, and get objects from S3.\nA MongoDB Atlas account. It is free to sign up, and there is a free tier that you can use to follow along with this post. The Community version of MongoDB does not include MongoDB Realm, so this post does not apply to it.\nEmulating an application environment # We\u0026rsquo;re going to have to setup a few things to make this look and feel like an environment a real application is using. Most modern applications are not just writing data to a database and reading it back; rather, they are consuming events from/publishing events to a message queue or stream, interacting with cloud storage, and relying on slight changes to data to kick off workflows. To keep things simple, our goal will be to monitor our database for new documents that are inserted and to upload a copy of them to S3 at the exact moment they are written to the database. In our theoretical application, we are using the copy of the document to do some additional processing or analysis with an AWS service that is reading from S3.\nSet up MongoDB Cluster # Log into your MongoDB Atlas account. Create a project to act as the container for your cluster, then create a cluster. If you want to use the free tier, choose the Shared category at the top of the Create Cluster page. Use a recommended, free-tier region such as us-east-1 or us-east-2 and then scroll down select the M0 Sandbox Cluster Tier. Change the name of the cluster from Cluster0 to Sandbox and click on Create Cluster.\nSet up the S3 Bucket # While the MongoDB cluster is creating, head over to the AWS console and login as your IAM user. Go to Amazon S3 in the AWS console and choose Create bucket. Give it a name like {yourname}-event-bucket and select the same region that you created your MongoDB Atlas cluster in. Leave the other options as the default selections and click Create bucket at the bottom of the page.\nSet up parameters in SSM # In the AWS console, search for AWS Systems Manager (SSM) and navigate to the service page. Once there, select Parameter Store on the left hand navigation, under Application Management. Select Create Parameter.\nOn the Create parameter page, enter event-target-bucket as the name for the parameter. Leave the Tier as Standard, the Type as String, and the Data type as text. Under value, enter the name of the S3 bucket you created in the previous step.\nConfiguring MongoDB Atlas and Realm # Load Data Into Cluster # Return to the MongoDB Atlas console. When your sandbox is finished provisioning, click the \u0026hellip; button and choose Load Sample Dataset. This will give us a few databases and collections of data to use for our testing purposes.\nSet up Realm App # While that data is being loaded, click on the Realm tab near the top of the screen. We want to select Create a New App and name it AWS-Event-App. Under \u0026ldquo;Link your Database\u0026rdquo;, select Use an existing MongoDB Atlas Data Source and choose the Sandbox cluster you created. Then click Create Realm Application.\nSet up Realm Values and Secrets # In order to hook into our AWS account and use the bucket and the parameter we just set up, we have to authenticate to the account from MongoDB Realm. Since we\u0026rsquo;ll be using our AWS IAM user\u0026rsquo;s access key and secret key, we don\u0026rsquo;t want to hardcode it into any code we end up writing or reading from source control. Therefore, we will use Realm\u0026rsquo;s Values feature to securely store these values.\nRealm\u0026rsquo;s Values feature allows you to store two types of values - regular values and secrets. Secrets cannot be directly accessed by the Realm API; instead, they must be mapped to their own regular value in order to be able to be retrieved. This prevents you from inadvertantly exposing a secret you did not intend to.\nClick on Values in the left hand navigation of your Realm app. Click on Create New Value. For the value\u0026rsquo;s name, enter AccessKeyID. Choose Value as the type, then for Content select Custom Content and paste in your Access Key wrapped in double quotations to mark it as a string. Then click on Save Draft.\nClick on Create New Value again. Enter SecretAccessKeySecret as the value name, and this time select Secret under Type. For the value of secret, paste in your AWS IAM user\u0026rsquo;s Secret Access Key, then click Save Draft.\nWe\u0026rsquo;ve entered the Secret Access Key, but we cannot directly access it via the Realm API since it is a secret. We now have to create a value and map it to that secret. Click on Create New Value a third time. Enter SecretAccessKey and select Value as the type. Under Add Content, choose Link to Secret and select the SecretAccessKeySecret you just created. Click Save Draft.\nWe have one more value to set up before we\u0026rsquo;re done. Click on Create New Value again, and this time enter Region as the value name. Leave Type as Value and Add Content as Custom Content. In the content field, enter the AWS region where you set up your S3 bucket and SSM parameter, again wrapped in double quotations. Then click Save Draft.\nYou may ask why we bothered with SSM at all to store the parameter of the S3 bucket name. It is true that we could also store the bucket name here in Realm\u0026rsquo;s values; however, for the scope of the post we will assume the application environment has already been using SSM for parameters such as these, and therefore it will be less work for us to leave it that way.\nCreate Realm Functions # Now we are ready to start writing some code. First, let\u0026rsquo;s bring in the AWS SDK which we will use in our code to make calls to the specific AWS services we will use. Select Functions in the left hand navigation then click the Dependencies tab at the top of the page. Select Add Dependency. On the modal that appears, enter aws-sdk as the package name. You can leave the version blank, but if you run into issues, you may want to come back and change the version to 2.737.0, which is what I used to write this post. When you\u0026rsquo;re done, click Add.\nThe aws-sdk node library should be added as a dependency. Now click Create New Function button in the upper right. In the Add Function page, enter MoveDocToQueue as the function and for now choose System under Authentication. Click on the Function Editor tab at the top, and paste this code in over what is currently there.\nexports = async function(event){ const AWS = require(\u0026#39;aws-sdk\u0026#39;); const config = { accessKeyId: context.values.get(\u0026#34;AccessKeyID\u0026#34;), secretAccessKey: context.values.get(\u0026#34;SecretAccessKey\u0026#34;), region: context.values.get(\u0026#34;Region\u0026#34;) }; const SSMparams = { Name: \u0026#39;event-target-bucket\u0026#39;, WithDecryption: false }; const doc = JSON.stringify(event.fullDocument); let SSM = new AWS.SSM(config); const ssmPromise = await SSM.getParameter(SSMparams).promise(); bucketName = ssmPromise.Parameter.Value; const S3params = { Bucket: bucketName, Key: \u0026#34;queue-\u0026#34; + event.fullDocument._id, Body: doc }; let S3 = new AWS.S3(config); const s3Promise = S3.putObject(S3params).promise(); s3Promise.then(function(data) { console.log(\u0026#39;Put Object Success\u0026#39;); }).catch(function(err) { console.log(err); }); }; Normally with Realm functions, you can click on Run near the bottom right to test out the function. However, since this function will require a change event in the database to run, we will need to map it to a trigger first and modify some data to test it out. For now, click on Save Draft.\nWe now have the actual code we want to run to move documents to our S3 bucket. But how do we get it to run whenever we insert data? The answer is a Realm Trigger. Select Triggers on the left hand navigation, then add a new trigger.\nOn the Add Trigger page, enter moveDocToQueueTrigger as the name, then select your Sandbox cluster under Trigger Source Details. For database, select sample_mflix and for collection select movies. Under Operation type, ensure Insert is checked as we want this trigger to fire on every new document inserted into the sample_mflix.movies collection. Keep scrolling down and ensure that Full Document is turned to ON. Under Select an Event Type, choose Function and select the MoveDocToQueue function that we just created. Then click Save.\nDeploying and Testing # We are now ready to try our first test! On the blue banner at the top of the page, you can now choose to Deploy the changes to your Realm app. The values, secrets, function, and trigger should now be ready to go. Open up a connection to your MongoDB cluster and insert a new document into the sample_mflix.movies collection. You can do this via the mongo shell, a MongoDB driver, MongoDB Compass, or just by using the Data Explorer built into Atlas to duplicate an existing document in that collection. Keep in mind that if you are using the shell, a driver, or Compass, you will need to add your current IP address to the IP Access list in your Atlas project.\nAfter you have inserted a new document, return to MongoDB Realm and select Logs in the left hand navigation. You should an entry in the History showing when your trigger was invoked. Expanding the entry will show you the logs from that particular function - if you set up your values and secrets correctly, you should see the log message \u0026ldquo;Put Object Success\u0026rdquo;, meaning that our attempt to put the document into the S3 bucket appears to be successful. (If you do not see this output, double check your values and secrets, your IAM user permissions, and your S3 and SSM configuration.)\nLet\u0026rsquo;s double check that this actually sent the document to S3 by going back to our AWS console and navigating to the S3 service page. Find the bucket that you created at the beginning of this process and open it up. You should see an object in there starting with queue- and then the _id value of the document you inserted. The object is named this way because we set this as the Key value when setting up the S3 parameters in our Realm function. If you see the document, then your setup is successful!\nYou can continue experimenting by bringing in other AWS services as well, such as sending the document to a Kinesis data stream instead of just an S3 bucket.\n","date":"March 20 2022","permalink":"/posts/2022-03-20/","section":"Posts","summary":"Disclaimer: I am a MongoDB employee.","title":"Building Event Driven Experiences with MongoDB Realm and AWS"},{"content":"","date":"March 20 2022","permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"NodeJS"},{"content":"About # Hello world!\nI\u0026rsquo;m John Misczak and this is my blog where I (infrequently) write about some stuff I\u0026rsquo;m working on or topics I\u0026rsquo;m currently learning about.\nI\u0026rsquo;ve taken on a number of different roles in my career, including in identity management, penetration testing, application security, threat modeling, and cloud security. I\u0026rsquo;ve even dipped my toe into the sales world for a bit to help customers build cool stuff. Currently, my focus is on helping teams build applications and services securely in public cloud environments.\nI always enjoy talking to new people so please reach out if you would like to say hello.\n","date":"January 1 0001","permalink":"/about/","section":"/misczak","summary":"About # Hello world!","title":""},{"content":"Contact Me # There are a few ways you can get in touch with me to discuss any of the topics I\u0026rsquo;ve written about here or anything else that comes to mind.\nEmail # john.misczak [at] gmail.com\nMastodon # https://infosec.exchange/@misczak\nLinkedIn # https://www.linkedin.com/in/johnmisczak\n","date":"January 1 0001","permalink":"/contact/","section":"/misczak","summary":"Contact Me # There are a few ways you can get in touch with me to discuss any of the topics I\u0026rsquo;ve written about here or anything else that comes to mind.","title":""},{"content":"","date":"January 1 0001","permalink":"/archive/","section":"/misczak","summary":"This page contains an archive of all posts.","title":"Archive"}]